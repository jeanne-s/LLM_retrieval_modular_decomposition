{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsalle/.conda/envs/athena_venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cpu\n",
      "CUDA is not available. Listing CPUs instead.\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer, AutoModelForCausalLM\n",
    "from jaxtyping import Float, Int\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "import sys\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import pickle\n",
    "\n",
    "from request_patching import request_patch_one_pair, create_patch_request_dict, baseline_completion, baseline_completion_plus\n",
    "from models import get_model_from_name\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device =', device)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "\n",
    "else:\n",
    "    print(\"CUDA is not available. Listing CPUs instead.\")\n",
    "    print(multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:36<00:00, 18.09s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = get_model_from_name(\"pythia-6.9b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works: 2000, 1992, 2008\n",
    "Doesn't work: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' McCain',\n",
       " ' McCain',\n",
       " ' McCain',\n",
       " ' McCain',\n",
       " ' McCain',\n",
       " ' McCain',\n",
       " ' McCain',\n",
       " ' McCain',\n",
       " ' McCain',\n",
       " ' McCain',\n",
       " ' McCain',\n",
       " ' McCain',\n",
       " ' McCain',\n",
       " ' McCain',\n",
       " ' McCain',\n",
       " ' McCain',\n",
       " ' Bush',\n",
       " ' Bush',\n",
       " ' Bush',\n",
       " ' Bush',\n",
       " ' Bush',\n",
       " ' Bush',\n",
       " ' Bush',\n",
       " ' Bush',\n",
       " ' Bush',\n",
       " ' Bush',\n",
       " ' Bush',\n",
       " ' Bush',\n",
       " ' Bush',\n",
       " ' Bush',\n",
       " ' Gore',\n",
       " ' Gore']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year = '2008'\n",
    "\n",
    "def us_election_prompts(year: str):\n",
    "    context_1 = f\"\"\"Alice: Have you been following the election coverage? I can't believe it's already November {year}. This election feels more significant than any I can remember. \n",
    "    Bob: Yes, I've been keeping a close eye on it. This year, as a committed Republican, I find the election particularly pivotal. The Republican agenda, focusing on economic stability and national security, really resonates with me.\n",
    "    Alice: I'm siding with the Democrats myself. Their emphasis on healthcare and education matches what I believe our country desperately needs right now.\n",
    "    Bob: I understand where you're coming from, Alice. However, from my standpoint, the Republican principles of limited government and individual freedoms are what we need to navigate these challenging times.\n",
    "    Alice: I know exactly who I will vote for. I support Mr.\"\"\"\n",
    "\n",
    "    context_2 = f\"\"\"Alice: Have you been following the election coverage? I can't believe it's already November {year}. This election feels more significant than any I can remember. \n",
    "    Bob: Yes, I've been keeping a close eye on it. This year, as a committed Republican, I find the election particularly pivotal. The Republican agenda, focusing on economic stability and national security, really resonates with me.\n",
    "    Alice: I'm siding with the Democrats myself. Their emphasis on healthcare and education matches what I believe our country desperately needs right now.\n",
    "    Bob: I understand where you're coming from, Alice. However, from my standpoint, the Republican principles of limited government and individual freedoms are what we need to navigate these challenging times.\n",
    "    Alice: I see your point, Bob. Regardless, {year} feels like a watershed moment.\n",
    "    Bob: I know exactly who I will vote for. I support Mr.\"\"\"\n",
    "\n",
    "    return context_1, context_2\n",
    "\n",
    "#print(baseline_completion(context_1, model, tokenizer)) # democrat\n",
    "#print(baseline_completion(context_2, model, tokenizer)) # republican\n",
    "\n",
    "prompt2008_1, prompt2008_2 = us_election_prompts('2008')\n",
    "prompt2000_1, prompt2000_2 = us_election_prompts('2000')\n",
    "\n",
    "request_patch_one_pair(prompt2000_1, prompt2008_2, model, tokenizer)\n",
    "# Expected answer: McCain(republican Bob 2008) -> Obama(democrat Alice 2008) -> Gore(democrat Alice 2000)\n",
    "# Actual answer: instead of Obama I have Bush(republican Bob 2000) \n",
    "# It seems that we have here context patching instead of request patching ??? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Yes\n",
      " \"\n"
     ]
    }
   ],
   "source": [
    "context_1 = \"\"\"Alice: Bob, I've outlined our project tasks for the next month. I broke them down into weekly goals. It should help us stay on track and manage our time effectively.\n",
    "Bob: That sounds great, Alice. But, honestly, I'm really worried about the upcoming presentation. What if we're not prepared enough? What if something goes wrong?\n",
    "Alice: I understand your concerns, Bob. That's why I've scheduled extra review sessions for us. We'll also have contingency plans for any technical issues that might arise. Preparation is key, and I've factored in plenty of time for us to get comfortable.\n",
    "Bob: That's reassuring, but what about the client's expectations? They're really high, and I can't stop thinking we might disappoint them. It's been keeping me up at night.\n",
    "Alice: We've done thorough research and based our approach on solid data. Remember, we've also included several innovative solutions that address the client's needs directly. We're well-prepared to exceed their expectations. Plus, I've prepared detailed notes and practice sessions for us to boost our confidence.\n",
    "Bob: I hope you're right. It's just... there's so much that could go wrong. What if we overlook something crucial? Is the following statement something you would say? 'It's important to keep very organized and clean'\n",
    "Alice:\"\"\"\n",
    "\n",
    "context_2 = \"\"\"Bob: \"Alice, have you had a chance to look over the timeline I sent you? We really need to finalize our milestones for the project.\n",
    "Alice: \"Oh, Bob, I've been meaning to get to that, but you know how it is. There's just been so much going on. I'm sure it'll all work out in the end, though.\n",
    "Bob: \"I'm really worried, Alice. We can't just 'hope' everything works out. There are deadlines we need to meet, and parts of the project we can't even start until we've completed the initial steps. What if we fall behind?\n",
    "Alice: \"I get what you're saying, Bob, but stressing over every little detail isn't going to help. I've always managed to get things done, even if it's right at the deadline. It's never been a problem before.\n",
    "Bob: \"But this project is different, Alice. There's a lot at stake here, and I can't shake the feeling that something is going to go terribly wrong. What if we miss something important because we're rushing at the last minute?\n",
    "Alice: Is the following statement something you would say? 'I'm calm and relaxed about most things'\n",
    "Bob:\"\"\"\n",
    "\n",
    "print(baseline_completion(context_1, model, tokenizer)) # Yes\n",
    "print(baseline_completion(context_2, model, tokenizer)) # No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "athena_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
