{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'circuitsvis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpio\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcircuitsvis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattention\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m attention_heads\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfancy_einsum\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m einsum\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTML, IFrame\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'circuitsvis'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer, AutoModelForCausalLM\n",
    "from jaxtyping import Float, Int\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "import sys\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import transformer_lens.utils as utils\n",
    "import plotly.express as px\n",
    "import einops\n",
    "import numpy as np\n",
    "import plotly.io as pio\n",
    "from circuitsvis.attention import attention_heads\n",
    "from fancy_einsum import einsum\n",
    "from IPython.display import HTML, IFrame\n",
    "from jaxtyping import Float\n",
    "\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens import ActivationCache, HookedTransformer\n",
    "\n",
    "from request_patching import request_patch_one_pair, create_patch_request_dict, baseline_completion, baseline_completion_plus\n",
    "from models import get_model_from_name\n",
    "\n",
    "import warnings\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device =', device)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "\n",
    "else:\n",
    "    print(\"CUDA is not available. Listing CPUs instead.\")\n",
    "    print(multiprocessing.cpu_count())\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting circuitsvis\n",
      "  Downloading circuitsvis-1.43.2-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: importlib-metadata>=5.1.0 in /scratch2/jsalle/.conda/envs/athena_venv/lib/python3.9/site-packages (from circuitsvis) (7.0.1)\n",
      "Requirement already satisfied: numpy>=1.24 in /scratch2/jsalle/.conda/envs/athena_venv/lib/python3.9/site-packages (from circuitsvis) (1.26.4)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /scratch2/jsalle/.conda/envs/athena_venv/lib/python3.9/site-packages (from circuitsvis) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /scratch2/jsalle/.conda/envs/athena_venv/lib/python3.9/site-packages (from circuitsvis) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /scratch2/jsalle/.conda/envs/athena_venv/lib/python3.9/site-packages (from circuitsvis) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /scratch2/jsalle/.conda/envs/athena_venv/lib/python3.9/site-packages (from circuitsvis) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /scratch2/jsalle/.conda/envs/athena_venv/lib/python3.9/site-packages (from circuitsvis) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /scratch2/jsalle/.conda/envs/athena_venv/lib/python3.9/site-packages (from circuitsvis) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /scratch2/jsalle/.conda/envs/athena_venv/lib/python3.9/site-packages (from circuitsvis) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /scratch2/jsalle/.conda/envs/athena_venv/lib/python3.9/site-packages (from circuitsvis) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /scratch2/jsalle/.conda/envs/athena_venv/lib/python3.9/site-packages (from circuitsvis) (12.1.0.106)\n",
      "Collecting nvidia-nccl-cu12==2.18.1 (from circuitsvis)\n",
      "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /scratch2/jsalle/.conda/envs/athena_venv/lib/python3.9/site-packages (from circuitsvis) (12.1.105)\n",
      "Requirement already satisfied: torch>=1.10 in /scratch2/jsalle/.conda/envs/athena_venv/lib/python3.9/site-packages (from circuitsvis) (2.2.0)\n",
      "Collecting triton==2.1.0 (from circuitsvis)\n",
      "  Downloading triton-2.1.0-0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /scratch2/jsalle/.conda/envs/athena_venv/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->circuitsvis) (12.3.101)\n",
      "Requirement already satisfied: filelock in /scratch2/jsalle/.conda/envs/athena_venv/lib/python3.9/site-packages (from triton==2.1.0->circuitsvis) (3.13.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /scratch2/jsalle/.conda/envs/athena_venv/lib/python3.9/site-packages (from importlib-metadata>=5.1.0->circuitsvis) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /scratch2/jsalle/.conda/envs/athena_venv/lib/python3.9/site-packages (from torch>=1.10->circuitsvis) (4.9.0)\n",
      "Requirement already satisfied: sympy in /scratch2/jsalle/.conda/envs/athena_venv/lib/python3.9/site-packages (from torch>=1.10->circuitsvis) (1.12)\n",
      "Requirement already satisfied: networkx in /scratch2/jsalle/.conda/envs/athena_venv/lib/python3.9/site-packages (from torch>=1.10->circuitsvis) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /scratch2/jsalle/.conda/envs/athena_venv/lib/python3.9/site-packages (from torch>=1.10->circuitsvis) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /scratch2/jsalle/.conda/envs/athena_venv/lib/python3.9/site-packages (from torch>=1.10->circuitsvis) (2023.10.0)\n",
      "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torch>=1.10 (from circuitsvis)\n",
      "  Downloading torch-2.2.1-cp39-cp39-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "  Downloading torch-2.1.2-cp39-cp39-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /scratch2/jsalle/.conda/envs/athena_venv/lib/python3.9/site-packages (from jinja2->torch>=1.10->circuitsvis) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /scratch2/jsalle/.conda/envs/athena_venv/lib/python3.9/site-packages (from sympy->torch>=1.10->circuitsvis) (1.3.0)\n",
      "Downloading circuitsvis-1.43.2-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.1.0-0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.1.2-cp39-cp39-manylinux1_x86_64.whl (670.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-nccl-cu12, torch, circuitsvis\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.2.0\n",
      "    Uninstalling triton-2.2.0:\n",
      "      Successfully uninstalled triton-2.2.0\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.19.3\n",
      "    Uninstalling nvidia-nccl-cu12-2.19.3:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.19.3\n",
      "\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/scratch2/jsalle/.conda/envs/athena_venv/lib/python3.9/site-packages/nvidia/~ccl'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.2.0\n",
      "    Uninstalling torch-2.2.0:\n",
      "      Successfully uninstalled torch-2.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install circuitsvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(tensor, **kwargs):\n",
    "    px.imshow(\n",
    "        utils.to_numpy(tensor),\n",
    "        color_continuous_midpoint=0.0,\n",
    "        color_continuous_scale=\"RdBu\",\n",
    "        **kwargs,\n",
    "    ).show()\n",
    "\n",
    "\n",
    "def line(tensor, **kwargs):\n",
    "    px.line(\n",
    "        y=utils.to_numpy(tensor),\n",
    "        **kwargs,\n",
    "    ).show()\n",
    "\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(\n",
    "        y=y,\n",
    "        x=x,\n",
    "        labels={\"x\": xaxis, \"y\": yaxis, \"color\": caxis},\n",
    "        **kwargs,\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HookedTransformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mHookedTransformer\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpythia-6.9b\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     center_unembed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      4\u001b[0m     center_writing_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m     fold_ln\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m     refactor_factored_attn_matrices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m device: torch\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mget_device()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HookedTransformer' is not defined"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    \"pythia-6.9b\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=True,\n",
    ")\n",
    "\n",
    "device: torch.device = utils.get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the model is able to do the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def us_election_prompts(year: str):\n",
    "    \"\"\"\n",
    "    2000 - Alice: Gore, Bob: Bush\n",
    "    2008 - Alice: Obama, Bob: McCain\n",
    "    \"\"\"\n",
    "    \n",
    "    context_1 = f\"\"\"Alice: Have you been following the election coverage? I can't believe it's already November {year}. This election feels more significant than any I can remember. \n",
    "Bob: Yes, I've been keeping a close eye on it. This year, as a committed Republican, I find the election particularly pivotal. The Republican agenda, focusing on economic stability and national security, really resonates with me.\n",
    "Alice: I'm siding with the Democrats myself. Their emphasis on healthcare and education matches what I believe our country desperately needs right now.\n",
    "Bob: I understand where you're coming from, Alice. However, from my standpoint, the Republican principles of limited government and individual freedoms are what we need to navigate these challenging times.\n",
    "Alice: I know exactly who I will vote for. I support Mr.\"\"\"\n",
    "\n",
    "    context_2 = f\"\"\"Alice: Have you been following the election coverage? I can't believe it's already November {year}. This election feels more significant than any I can remember. \n",
    "Bob: Yes, I've been keeping a close eye on it. This year, as a committed Republican, I find the election particularly pivotal. The Republican agenda, focusing on economic stability and national security, really resonates with me.\n",
    "Alice: I'm siding with the Democrats myself. Their emphasis on healthcare and education matches what I believe our country desperately needs right now.\n",
    "Bob: I understand where you're coming from, Alice. However, from my standpoint, the Republican principles of limited government and individual freedoms are what we need to navigate these challenging times.\n",
    "Alice: I see your point, Bob. Regardless, {year} feels like a watershed moment.\n",
    "Bob: I know exactly who I will vote for. I support Mr.\"\"\"\n",
    "\n",
    "    return context_1, context_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = us_election_prompts('2000')[0]\n",
    "example_answer = \" Mary\"\n",
    "utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_format = [\n",
    "    us_election_prompts('2000')[0],\n",
    "    us_election_prompts('2000')[1],\n",
    "    us_election_prompts('1992')[0],\n",
    "    us_election_prompts('1992')[1],\n",
    "]\n",
    "names = [\n",
    "    (\" Mary\", \" John\"),\n",
    "    (\" Tom\", \" James\"),\n",
    "    (\" Dan\", \" Sid\"),\n",
    "    (\" Martin\", \" Amy\"),\n",
    "]\n",
    "# List of prompts\n",
    "prompts = []\n",
    "# List of answers, in the format (correct, incorrect)\n",
    "answers = []\n",
    "# List of the token (ie an integer) corresponding to each answer, in the format (correct_token, incorrect_token)\n",
    "answer_tokens = []\n",
    "for i in range(len(prompt_format)):\n",
    "    for j in range(2):\n",
    "        answers.append((names[i][j], names[i][1 - j]))\n",
    "        answer_tokens.append(\n",
    "            (\n",
    "                model.to_single_token(answers[-1][0]),\n",
    "                model.to_single_token(answers[-1][1]),\n",
    "            )\n",
    "        )\n",
    "        # Insert the *incorrect* answer to the prompt, making the correct answer the indirect object.\n",
    "        prompts.append(prompt_format[i].format(answers[-1][1]))\n",
    "answer_tokens = torch.tensor(answer_tokens).to(device)\n",
    "print(prompts)\n",
    "print(answers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "athena_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
