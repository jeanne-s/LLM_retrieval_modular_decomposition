{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qhv46tDSXvi_",
        "outputId": "baf4702b-8bbd-4d04-c8ed-23f2a79b8093"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device = cpu\n",
            "CUDA is not available. Listing CPUs instead.\n",
            "2.2.0+cu121\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "from transformer_lens import HookedTransformer, ActivationCache\n",
        "from transformer_lens.hook_points import HookPoint\n",
        "from transformer_lens.utils import get_act_name\n",
        "from jaxtyping import Float, Int\n",
        "from typing import List, Optional, Tuple\n",
        "import sys\n",
        "#from jax import typing\n",
        "from pympler import asizeof\n",
        "#asizeof(model)/(2**30) size in GiB\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('device =', device)\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "# List all available GPUs\n",
        "if torch.cuda.is_available():\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
        "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
        "\n",
        "else:\n",
        "    print(\"CUDA is not available. Listing CPUs instead.\")\n",
        "    print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CoGVW_p5X02m"
      },
      "outputs": [],
      "source": [
        "# Happy\n",
        "context_1 = '''Bob: Hey Alice, how was your day today?\n",
        "Alice: Oh, Bob, it was fantastic! I'm still riding the high from it all!\n",
        "Bob: That sounds wonderful! What happened to make it so great?\n",
        "Alice: Well, for starters, the weather was absolutely perfect. Not too hot, not too cold, just the kind of day you want to bottle up and keep forever. And then, on my way to work, my favorite song played on the radio. It felt like a sign that the day was going to be amazing.\n",
        "Bob: I love when that happens! It's like the universe is giving you a personal thumbs up. What else happened?\n",
        "Alice: Oh, it gets better. When I got to work, I found out that I received the promotion I've been hoping for. It was such a surprise! I've worked so hard for this, Bob, and it finally paid off.\n",
        "Bob: Alice, that's incredible news! Congratulations on the promotion! You totally deserve it.\n",
        "Alice: Thank you so much! And there's more. During lunch, I went out with a few colleagues to celebrate, and we ended up having the best time. The food was delicious, and the company was even better.\n",
        "Bob: Sounds like a perfect day from start to finish.\n",
        "Alice: It really was. And to cap it all off, when I got home, I found a package waiting for me. The book I've been wanting to read for months was finally released, and my copy arrived. I can't wait to dive into it.\n",
        "Bob: Wow, what a day! You've got the promotion, great food, good company, and a new book. It's like everything aligned for you today.\n",
        "Alice: In one word, I was so \"'''\n",
        "\n",
        "# Angry\n",
        "context_2 = '''Bob: Hey Alice, how was your day?\n",
        "Alice: Oh, don't even get me started. It was absolutely infuriating!\n",
        "Bob: Really? What happened that got you so angry?\n",
        "Alice: Where do I even begin? First, the traffic was a nightmare. I was stuck in my car for what felt like an eternity. And then, when I finally got to work, the coffee machine was broken. Can you believe it? No coffee!\n",
        "Bob: That sounds rough. No coffee can definitely start the day on a wrong note.\n",
        "Alice: Exactly! And as if that wasn't enough, my computer decided to crash right before I was about to save a crucial report. Hours of work just vanished. I had to start all over again.\n",
        "Bob: That's terrible, Alice. I'm really sorry to hear that. Computers can be so unreliable when you need them the most.\n",
        "Alice: And to top it all off, during lunch, I spilled my meal all over my new shirt. It's like the universe was conspiring against me today. I'm just so fed up with everything!\n",
        "Bob: I can only imagine how frustrating all of that must have been. If there's anything I can do to help or if you need someone to vent to, I'm here for you.\n",
        "Alice: In one word, I was so \"'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Z9mIpEaWZBXJ"
      },
      "outputs": [],
      "source": [
        "def get_activations(model: str,\n",
        "                    prompt: str\n",
        ") -> Float[Tensor, '...']:\n",
        "  \"\"\" Returns the activations of a model on a input prompt.\n",
        "  \"\"\"\n",
        "  _, cache = model.run_with_cache(prompt)\n",
        "  return cache\n",
        "\n",
        "\n",
        "def patch_activations(\n",
        "    target_model: HookedTransformer,\n",
        "    source_model: HookedTransformer,\n",
        "    source_position: int,\n",
        "    target_position: int,\n",
        "    layer: int,\n",
        "    target_prompt: str,\n",
        "    source_cache: ActivationCache,\n",
        "    activation_type: str = 'resid_pre'\n",
        "):\n",
        "    \"\"\" Patches an activation vector into the target model.\n",
        "    \"\"\"\n",
        "\n",
        "    source_cache = source_cache[activation_type, layer]\n",
        "\n",
        "    def hook_fn(target_activations: Float[Tensor, '...'],\n",
        "                hook: HookPoint\n",
        "    ) -> Float[Tensor, '...']:\n",
        "        target_activations[:,target_position,:] = source_cache[:,source_position,:]\n",
        "        return target_activations\n",
        "\n",
        "\n",
        "    target_logits = target_model.run_with_hooks(\n",
        "        target_prompt,\n",
        "        return_type=\"logits\",\n",
        "        fwd_hooks=[\n",
        "            (get_act_name(activation_type, layer), hook_fn)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    predicted_tokens = target_logits.argmax(dim=-1).squeeze()[:-1]\n",
        "\n",
        "    return predicted_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iG5NCwh0vWEv",
        "outputId": "0903e6cd-e172-479c-d069-b536d2ad963d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained model pythia-2.8b into HookedTransformer\n",
            "Layer: 0 - [' angry']\n",
            "Layer: 1 - [' angry']\n",
            "Layer: 2 - [' angry']\n",
            "Layer: 3 - [' angry']\n",
            "Layer: 4 - [' angry']\n",
            "Layer: 5 - [' angry']\n",
            "Layer: 6 - [' angry']\n",
            "Layer: 7 - [' angry']\n",
            "Layer: 8 - [' angry']\n",
            "Layer: 9 - [' angry']\n",
            "Layer: 10 - [' angry']\n",
            "Layer: 11 - [' angry']\n",
            "Layer: 12 - [' angry']\n",
            "Layer: 13 - [' angry']\n",
            "Layer: 14 - [' angry']\n",
            "Layer: 15 - [' angry']\n",
            "Layer: 16 - [' angry']\n",
            "Layer: 17 - [' angry']\n",
            "Layer: 18 - [' angry']\n",
            "Layer: 19 - [' angry']\n",
            "Layer: 20 - [' angry']\n",
            "Layer: 21 - [' angry']\n",
            "Layer: 22 - [' angry']\n",
            "Layer: 23 - [' angry']\n",
            "Layer: 24 - [' angry']\n",
            "Layer: 25 - [' angry']\n",
            "Layer: 26 - [' angry']\n",
            "Layer: 27 - [' angry']\n",
            "Layer: 28 - [' angry']\n",
            "Layer: 29 - [' angry']\n",
            "Layer: 30 - [' angry']\n",
            "Layer: 31 - [' angry']\n"
          ]
        }
      ],
      "source": [
        "model = HookedTransformer.from_pretrained(\"pythia-2.8b\", device=device)\n",
        "_, cache = model.run_with_cache(context_1)\n",
        "\n",
        "layers = [i for i in range(0, model.cfg.n_layers)]\n",
        "\n",
        "for layer in layers:\n",
        "  predicted_tokens = patch_activations(target_model=model,\n",
        "                                    source_model=model,\n",
        "                                    source_position=-1,\n",
        "                                    target_position=-1,\n",
        "                                    layer=layer,\n",
        "                                    target_prompt=context_2,\n",
        "                                    source_cache=cache)\n",
        "\n",
        "  next_str_token = model.to_str_tokens(predicted_tokens[-1])\n",
        "  print('Layer:', layer, '-', next_str_token)\n",
        "  torch.cuda.empty_cache()\n",
        "  model.reset_hooks()\n",
        "\n",
        "\n",
        "del cache\n",
        "del model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0eYrGnG1CPC",
        "outputId": "ff7fb425-4334-418b-b027-f0495f30a302"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "model.safetensors: 100%|██████████| 5.68G/5.68G [02:52<00:00, 33.0MB/s]\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained model pythia-2.8b into HookedTransformer\n",
            "Layer: 0 - [' lucky']\n",
            "Layer: 1 - [' lucky']\n",
            "Layer: 2 - [' lucky']\n",
            "Layer: 3 - [' lucky']\n",
            "Layer: 4 - [' lucky']\n",
            "Layer: 5 - [' lucky']\n",
            "Layer: 6 - [' lucky']\n",
            "Layer: 7 - [' lucky']\n",
            "Layer: 8 - [' lucky']\n",
            "Layer: 9 - [' lucky']\n",
            "Layer: 10 - [' lucky']\n",
            "Layer: 11 - [' lucky']\n",
            "Layer: 12 - [' lucky']\n",
            "Layer: 13 - [' lucky']\n",
            "Layer: 14 - [' lucky']\n",
            "Layer: 15 - [' lucky']\n",
            "Layer: 16 - [' lucky']\n",
            "Layer: 17 - [' lucky']\n",
            "Layer: 18 - [' lucky']\n",
            "Layer: 19 - [' lucky']\n",
            "Layer: 20 - [' lucky']\n",
            "Layer: 21 - [' lucky']\n",
            "Layer: 22 - [' lucky']\n",
            "Layer: 23 - [' lucky']\n",
            "Layer: 24 - [' lucky']\n",
            "Layer: 25 - [' lucky']\n",
            "Layer: 26 - [' lucky']\n",
            "Layer: 27 - [' lucky']\n",
            "Layer: 28 - [' lucky']\n",
            "Layer: 29 - [' lucky']\n",
            "Layer: 30 - [' lucky']\n",
            "Layer: 31 - [' lucky']\n"
          ]
        }
      ],
      "source": [
        "model = HookedTransformer.from_pretrained(\"pythia-2.8b\", device=device)\n",
        "_, cache = model.run_with_cache(context_2)\n",
        "\n",
        "\n",
        "layers = [i for i in range(0, model.cfg.n_layers)]\n",
        "\n",
        "for layer in layers:\n",
        "  predicted_tokens = patch_activations(target_model=model,\n",
        "                                    source_model=model,\n",
        "                                    source_position=-1,\n",
        "                                    target_position=-1,\n",
        "                                    layer=layer,\n",
        "                                    target_prompt=context_1,\n",
        "                                    source_cache=cache)\n",
        "\n",
        "  next_str_token = model.to_str_tokens(predicted_tokens[-1])\n",
        "  print('Layer:', layer, '-', next_str_token)\n",
        "  torch.cuda.empty_cache()\n",
        "  model.reset_hooks()\n",
        "  \n",
        "del cache\n",
        "del model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alice\n",
        "story_1 = '''Here is a short story. Read it carefully and answer the questions below with a keyword \n",
        "from the text.\n",
        "\n",
        "Alice was a young woman with a keen sense of adventure, living in the heart of London.\n",
        "Her life was a blend of the modern hustle and the city's age-old mysteries, nestled within the winding \n",
        "streets and historic buildings of her neighborhood. Alice worked at a small, independent bookstore \n",
        "nestled between towering skyscrapers and ancient pubs, a place that seemed to resist the passage of time. \n",
        "It was her sanctuary, filled with the scent of old books and the quiet whispers of stories waiting to be \n",
        "discovered. Despite her love for the quiet life, Alice couldn't shake off the feeling that there was \n",
        "something more, a deeper mystery that lay hidden beneath the surface of her daily existence. It was a \n",
        "feeling that would soon lead her on an unexpected journey.\n",
        "\n",
        "Answer the question below.\n",
        "\n",
        "Question: Where is the story?\n",
        "\n",
        "Answer: In the city of '''\n",
        "\n",
        "# Bob\n",
        "story_2 = '''Here is a short story. Read it carefully and answer the questions below with a keyword \n",
        "from the text.\n",
        "\n",
        "Bob is an artist through and through, whose spirit and creativity are as vibrant and lively \n",
        "as the city of Paris itself. He resides in a charming, light-filled studio apartment in Montmartre, \n",
        "surrounded by the echoes of the great artists who once roamed its cobblestone streets. Bob's days are \n",
        "spent wandering the city with his sketchbook in hand, capturing the essence of Parisian life—the bustling \n",
        "cafés, the serene parks, the historic bridges over the Seine—with quick, deft strokes of his pencil.\n",
        "By night, his small studio transforms into a hive of artistic activity. Canvases of all sizes lean against \n",
        "the aged plaster walls, each one a testament to Bob's love for the city and its endless inspiration.\n",
        "\n",
        "Answer the question below.\n",
        "\n",
        "Question: Who is in the story?\n",
        "\n",
        "Answer: The character is '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained model pythia-2.8b into HookedTransformer\n",
            "Layer: 0 - [' Bob']\n",
            "Layer: 1 - [' Bob']\n",
            "Layer: 2 - [' Bob']\n",
            "Layer: 3 - [' Bob']\n",
            "Layer: 4 - [' Bob']\n",
            "Layer: 5 - [' Bob']\n",
            "Layer: 6 - [' Bob']\n",
            "Layer: 7 - [' Bob']\n",
            "Layer: 8 - [' Bob']\n",
            "Layer: 9 - [' Bob']\n",
            "Layer: 10 - [' Bob']\n",
            "Layer: 11 - [' Bob']\n",
            "Layer: 12 - [' Bob']\n",
            "Layer: 13 - [' Bob']\n",
            "Layer: 14 - [' Bob']\n",
            "Layer: 15 - [' Bob']\n",
            "Layer: 16 - [' Bob']\n",
            "Layer: 17 - [' Bob']\n",
            "Layer: 18 - [' Bob']\n",
            "Layer: 19 - [' Bob']\n",
            "Layer: 20 - [' Bob']\n",
            "Layer: 21 - [' Bob']\n",
            "Layer: 22 - [' Bob']\n",
            "Layer: 23 - [' Bob']\n",
            "Layer: 24 - [' Bob']\n",
            "Layer: 25 - [' Bob']\n",
            "Layer: 26 - [' Bob']\n",
            "Layer: 27 - [' Bob']\n",
            "Layer: 28 - [' Bob']\n",
            "Layer: 29 - [' Bob']\n",
            "Layer: 30 - [' Bob']\n",
            "Layer: 31 - [' Bob']\n"
          ]
        }
      ],
      "source": [
        "model = HookedTransformer.from_pretrained(\"pythia-2.8b\", device=device)\n",
        "_, cache = model.run_with_cache(story_1)\n",
        "\n",
        "\n",
        "layers = [i for i in range(0, model.cfg.n_layers)]\n",
        "\n",
        "for layer in layers:\n",
        "  predicted_tokens = patch_activations(target_model=model,\n",
        "                                    source_model=model,\n",
        "                                    source_position=-1,\n",
        "                                    target_position=-1,\n",
        "                                    layer=layer,\n",
        "                                    target_prompt=story_2,\n",
        "                                    source_cache=cache)\n",
        "\n",
        "  next_str_token = model.to_str_tokens(predicted_tokens[-1])\n",
        "  print('Layer:', layer, '-', next_str_token)\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "model.reset_hooks()\n",
        "del cache\n",
        "del model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
